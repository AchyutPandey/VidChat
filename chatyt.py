# -*- coding: utf-8 -*-
"""ChatYT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BCS1RSZPwzZWcAYQP9_v-JNZENhoR_WV

# ChatYT
This notebook enables you to


*  Summarise YouTube videos
*  Ask questions about the topics discussed in the video

Please connect to a GPU runtime for faster execution.

You will need:


*  The url of the YouTube video
*  Your Gemini API Key
"""

!pip install yt-dlp
!pip install -q openai-whisper
!apt-get install -y ffmpeg
!pip install langchain
!pip install langchain-huggingface sentence-transformers langchain-chroma
!pip install langchain-community
!pip install langchain-openai
!pip install google-generativeai

"""Imports

"""

import yt_dlp
import whisper
import os
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from transformers import BartTokenizer, BartForConditionalGeneration
import google.generativeai as genai
from langchain.prompts import ChatPromptTemplate
from transformers import pipeline
from google.colab import userdata

genai.configure(api_key=userdata.get('GEMINI_API_KEY'))

url = "https://www.youtube.com/watch?v=1tRTWwZ5DIc"

def download_audio(link, file_name='audio.mp3'):
    with yt_dlp.YoutubeDL({'extract_audio': True,
                           'format': 'worstaudio',
                           'overwrites': True,
                           'outtmpl': file_name}) as video:
        info_dict = video.extract_info(link, download = True)
        video_title = info_dict['title']
        #video.download(link)
    return file_name

def download_subtitles(link, lang="en"):
    ydl_opts = {
        'writesubtitles': True,       # Enable subtitle download
        'subtitleslangs': ['en'],     # Language of subtitles
        'skip_download': True,        # Skip video/audio, only download subs
        'outtmpl': 'subtitles.%(ext)s'  # Save as subtitles.vtt (or srt if available)
    }
    return

# download_subtitles(url)

# download_audio(url)

def compress_audio(input_file, output_file="compressed.mp3"):
    os.system(f"ffmpeg -y -i {input_file} -ar 16000 -ac 1 {output_file}")
    return output_file

summ_model = whisper.load_model("medium")

def speech_to_text(audio_file):
    result = summ_model.transcribe(audio_file, task="translate")
    return result["text"]

# text = speech_to_text("compress_audio.mp3")

#langchain patr
# doc = Document(page_content=text, metadata={"source": "youtube"})
# print(doc.page_content[:100])

# splitter = RecursiveCharacterTextSplitter(
#     chunk_size=2000,
#     chunk_overlap=500,
# )

# chunks = splitter.split_documents([doc])

# print(len(chunks))
# print(chunks[0].page_content)

def generate_embeddings(text):
    doc = Document(page_content=text, metadata={"source": "youtube"})
    # print(doc.page_content[:100])
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
    )

    chunks = splitter.split_documents([doc])
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    db = Chroma.from_documents(chunks, embeddings, persist_directory="./chroma_db")
    return db

def closest(query, db):
    results = db.similarity_search(query, k=3)
    if len(results)==0:
        print("No matching results...")
        return
    return results

# embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
# db = Chroma.from_documents(docs, embeddings, persist_directory="./chroma_db")

# PROMPT = """Answer the following questions based only on the following context:
# {context}
# ---
# Answer the question based on the above context:
# {que}
# """

# context_text = "\n\n---\n\n".join(doc.page_content for doc, _score in results)
# prompt_template = ChatPromptTemplate.from_template(PROMPT)
# prompt = prompt_template.format(context = context_text, question = query_text)

def create_prompt(results, question):
    PROMPT = """Answer the following questions based only on the following context:
    {context}
    ---
    Answer the question based on the above context:
    {que}
    """
    if not results:
      return "Sorry, I couldn’t find anything relevant."

    context_text = "\n\n---\n\n".join(
        doc.page_content if not isinstance(doc, tuple) else doc[0].page_content
        for doc in results
    )

    prompt_template = ChatPromptTemplate.from_template(PROMPT)
    prompt = prompt_template.format(context = context_text, que = question)
    return prompt

def answer__llm(question, closest_chunks):
    model = genai.GenerativeModel("gemini-2.5-pro")
    prompt = create_prompt(closest_chunks, question)
    response =  model.generate_content(prompt)
    # return response_text
    # response = model.generate_content(question)

    if response.candidates and response.candidates[0].content.parts:
        answer = response.candidates[0].content.parts[0].text
        #print(answer)
        return answer
    else:
        return "No answer generated."
        #return None

def complete(video_url):
    audio_file = download_audio(video_url)
    compressed_audio = compress_audio(input_file = audio_file)
    transcript = speech_to_text(compressed_audio)
    generate_embeddings(transcript)

def preparation(url):
    audio_file = download_audio(url)
    compressed_audio = compress_audio(input_file = audio_file)
    transcript = speech_to_text(compressed_audio)
    return transcript

# model_name = "sshleifer/distilbart-cnn-12-6"

def download_summariser(model_name):
    from huggingface_hub import hf_hub_download
    model_dir = hf_hub_download(repo_id=model_name, filename="config.json", cache_dir="models")
    summarizer = pipeline("summarization", model=model_name, cache_dir="models")
    return summarizer

def load_model_and_tokenizer(model_name):
    tokenizer = BartTokenizer.from_pretrained(model_name)
    model = BartForConditionalGeneration.from_pretrained(model_name)
    return tokenizer, model

# def summarize(text, maxSummarylength=1000):
#     model_name = "sshleifer/distilbart-cnn-12-6"
#     tokenizer, model = load_model_and_tokenizer(model_name)
#     inputs = tokenizer.encode("summarize: " + text,
#                               return_tensors="pt",
#                               max_length=1024,
#                               truncation=True)
#     summary_ids = model.generate(
#         inputs,
#         max_length=int(maxSummarylength),
#         min_length=int(maxSummarylength / 5),
#         length_penalty=10.0,
#         num_beams=4,
#         early_stopping=True
#     )
#     summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
#     return summary
def summarize(text, max_summary_length=200, min_summary_length=50):
    model_name = "sshleifer/distilbart-cnn-12-6"
    tokenizer, model = load_model_and_tokenizer(model_name)

    inputs = tokenizer.encode(
        "summarize: " + text,
        return_tensors="pt",
        max_length=1024,
        truncation=True
    )

    summary_ids = model.generate(
        inputs,
        max_length=max_summary_length,
        min_length=min_summary_length,
        length_penalty=2.0,   # balanced
        num_beams=4,
        early_stopping=True
    )

    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# def recursive_summarize(text, chunk_size=800):
#     words = text.split()
#     chunks = [" ".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]

#     partial_summaries = []
#     for chunk in chunks:
#         partial_summaries.append(summarize(chunk))

#     return summarize(" ".join(partial_summaries))
def recursive_summarize(text, chunk_size=500):
    words = text.split()
    chunks = [" ".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]

    partial_summaries = [summarize(chunk) for chunk in chunks]


    if len(" ".join(partial_summaries).split()) < 900:
        return summarize(" ".join(partial_summaries), max_summary_length=250)
    else:
        return " ".join(partial_summaries)

def adaptive_recursive_summarize(text, chunk_size=800, min_words=150, max_words=400):
    """
    Adaptive summarizer that produces summaries of reasonable length automatically.
    """
    # Step 1: Split into chunks
    words = text.split()
    chunks = [" ".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]

    partial_summaries = [summarize(chunk, max_summary_length=max_words) for chunk in chunks]

    combined_summary = " ".join(partial_summaries)

    # Step 2: Adaptive refinement
    word_count = len(combined_summary.split())

    if word_count < min_words:
        # Too short → expand by re-summarizing original text with higher length
        return summarize(text, max_summary_length=max_words * 2)

    elif word_count > max_words * 3:
        # Too long → compress again
        return summarize(combined_summary, max_summary_length=max_words)

    else:
        # Good length → return combined
        return combined_summary

def summarize_functionality(text, loaded):
    if not loaded:
        model_name = "sshleifer/distilbart-cnn-12-6"
        download_summariser(model_name)
        tokenizer, model = load_model_and_tokenizer(model_name)
    return adaptive_recursive_summarize(text)

text = preparation(url)

# print(text)

def summarize_url(text):
    loaded = False
    summary = summarize_functionality(text, loaded)
    loaded = True
    return summary

print(summarize_url(text))

db = generate_embeddings(text)

question = "tell me about the rise on nvidia"

def qna_functionality(question):
    res = closest(question, db)
    answer = answer__llm(question, res)
    # if answer !=None:
    print(answer)

qna_functionality(question)

# for m in genai.list_models():
#     print(m.name, m.supported_generation_methods)

