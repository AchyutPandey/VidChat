{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ChatYT\n",
        "This notebook enables you to\n",
        "\n",
        "\n",
        "*  Summarise YouTube videos\n",
        "*  Ask questions about the topics discussed in the video\n",
        "\n",
        "Please connect to a GPU runtime for faster execution.\n",
        "\n",
        "You will need:\n",
        "\n",
        "\n",
        "*  The url of the YouTube video\n",
        "*  Your Gemini API Key\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ehiocd0Vr3B4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBbl_ksacfYK",
        "outputId": "d8d76585-2626-4f72-98fb-dc7821f27816",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.12/dist-packages (2025.8.27)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.75)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.16)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Collecting langchain-chroma\n",
            "  Downloading langchain_chroma-0.2.5-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.3.75)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.21.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.34.4)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.55.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from langchain-chroma) (2.0.2)\n",
            "Collecting chromadb>=1.0.9 (from langchain-chroma)\n",
            "  Downloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (2.11.7)\n",
            "Collecting pybase64>=1.4.1 (from chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.35.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.36.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.36.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.74.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.16.1)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (3.11.2)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2.32.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.8)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.16)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb>=1.0.9->langchain-chroma) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.27.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.5.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.24.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (5.29.5)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb>=1.0.9->langchain-chroma) (0.57b0)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (3.4.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (1.1.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (15.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.6.1)\n",
            "Downloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
            "Downloading langchain_chroma-0.2.5-py3-none-any.whl (12 kB)\n",
            "Downloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m125.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m129.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=546f07da24f4d4a9fa49c7a9899a7234695ad653c0d147351bcef5dca6654d40\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, pybase64, overrides, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, coloredlogs, onnxruntime, kubernetes, opentelemetry-exporter-otlp-proto-grpc, langchain-huggingface, chromadb, langchain-chroma\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.20 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 langchain-chroma-0.2.5 langchain-huggingface-0.3.1 mmh3-5.2.0 onnxruntime-1.22.1 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 overrides-7.7.0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 uvloop-0.21.0 watchfiles-1.1.0\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.29)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.75)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.16)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.24.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.32)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.74 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.3.75)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.101.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.4.16)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.99.9->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain-openai) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.5.0)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.179.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install yt-dlp\n",
        "!pip install -q openai-whisper\n",
        "!apt-get install -y ffmpeg\n",
        "!pip install langchain\n",
        "!pip install langchain-huggingface sentence-transformers langchain-chroma\n",
        "!pip install langchain-community\n",
        "!pip install langchain-openai\n",
        "!pip install google-generativeai\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports\n"
      ],
      "metadata": {
        "id": "aTZcPkP1rqpb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgXrf5uzcjS-"
      },
      "outputs": [],
      "source": [
        "import yt_dlp\n",
        "import whisper\n",
        "import os\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import google.generativeai as genai\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from transformers import pipeline\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=userdata.get('GEMINI_API_KEY'))"
      ],
      "metadata": {
        "id": "VkMMouW5whDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZsM9-lhXtfB"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.youtube.com/watch?v=1tRTWwZ5DIc\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsX0e7NBYF9r"
      },
      "outputs": [],
      "source": [
        "def download_audio(link, file_name='audio.mp3'):\n",
        "    with yt_dlp.YoutubeDL({'extract_audio': True,\n",
        "                           'format': 'worstaudio',\n",
        "                           'overwrites': True,\n",
        "                           'outtmpl': file_name}) as video:\n",
        "        info_dict = video.extract_info(link, download = True)\n",
        "        video_title = info_dict['title']\n",
        "        #video.download(link)\n",
        "    return file_name"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_subtitles(link, lang=\"en\"):\n",
        "    ydl_opts = {\n",
        "        'writesubtitles': True,       # Enable subtitle download\n",
        "        'subtitleslangs': ['en'],     # Language of subtitles\n",
        "        'skip_download': True,        # Skip video/audio, only download subs\n",
        "        'outtmpl': 'subtitles.%(ext)s'  # Save as subtitles.vtt (or srt if available)\n",
        "    }\n",
        "    return"
      ],
      "metadata": {
        "id": "OWeHEjOZ19Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download_subtitles(url)"
      ],
      "metadata": {
        "id": "8XubFdr61_Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p6RNrTsZ18b"
      },
      "outputs": [],
      "source": [
        "# download_audio(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48vz2M0IFwCX"
      },
      "outputs": [],
      "source": [
        "def compress_audio(input_file, output_file=\"compressed.mp3\"):\n",
        "    os.system(f\"ffmpeg -y -i {input_file} -ar 16000 -ac 1 {output_file}\")\n",
        "    return output_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptLS7cjbHK5N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e80f138f-c9df-4efd-9493-4155f07b8ea2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [01:28<00:00, 17.4MiB/s]\n"
          ]
        }
      ],
      "source": [
        "summ_model = whisper.load_model(\"medium\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCCNLInMHMYW"
      },
      "outputs": [],
      "source": [
        "def speech_to_text(audio_file):\n",
        "    result = summ_model.transcribe(audio_file, task=\"translate\")\n",
        "    return result[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMPJdG2FHYT5"
      },
      "outputs": [],
      "source": [
        "# text = speech_to_text(\"compress_audio.mp3\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#langchain patr\n",
        "# doc = Document(page_content=text, metadata={\"source\": \"youtube\"})\n",
        "# print(doc.page_content[:100])"
      ],
      "metadata": {
        "id": "cC5Fbij3CLa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitter = RecursiveCharacterTextSplitter(\n",
        "#     chunk_size=2000,\n",
        "#     chunk_overlap=500,\n",
        "# )\n",
        "\n",
        "# chunks = splitter.split_documents([doc])\n",
        "\n",
        "# print(len(chunks))\n",
        "# print(chunks[0].page_content)"
      ],
      "metadata": {
        "id": "hyJ_Elz8C183"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embeddings(text):\n",
        "    doc = Document(page_content=text, metadata={\"source\": \"youtube\"})\n",
        "    # print(doc.page_content[:100])\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "    )\n",
        "\n",
        "    chunks = splitter.split_documents([doc])\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    db = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_db\")\n",
        "    return db"
      ],
      "metadata": {
        "id": "J9n0a5v5J3ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def closest(query, db):\n",
        "    results = db.similarity_search(query, k=3)\n",
        "    if len(results)==0:\n",
        "        print(\"No matching results...\")\n",
        "        return\n",
        "    return results"
      ],
      "metadata": {
        "id": "DPfC4mtoOWpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "# db = Chroma.from_documents(docs, embeddings, persist_directory=\"./chroma_db\")"
      ],
      "metadata": {
        "id": "DdBWYDICHZop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PROMPT = \"\"\"Answer the following questions based only on the following context:\n",
        "# {context}\n",
        "# ---\n",
        "# Answer the question based on the above context:\n",
        "# {que}\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "YO-0lc_4W-gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# context_text = \"\\n\\n---\\n\\n\".join(doc.page_content for doc, _score in results)\n",
        "# prompt_template = ChatPromptTemplate.from_template(PROMPT)\n",
        "# prompt = prompt_template.format(context = context_text, question = query_text)"
      ],
      "metadata": {
        "id": "jv-sEByuYasY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(results, question):\n",
        "    PROMPT = \"\"\"Answer the following questions based only on the following context:\n",
        "    {context}\n",
        "    ---\n",
        "    Answer the question based on the above context:\n",
        "    {que}\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "      return \"Sorry, I couldn’t find anything relevant.\"\n",
        "\n",
        "    context_text = \"\\n\\n---\\n\\n\".join(\n",
        "        doc.page_content if not isinstance(doc, tuple) else doc[0].page_content\n",
        "        for doc in results\n",
        "    )\n",
        "\n",
        "    prompt_template = ChatPromptTemplate.from_template(PROMPT)\n",
        "    prompt = prompt_template.format(context = context_text, que = question)\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "H7T0pv8AgMyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer__llm(question, closest_chunks):\n",
        "    model = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
        "    prompt = create_prompt(closest_chunks, question)\n",
        "    response =  model.generate_content(prompt)\n",
        "    # return response_text\n",
        "    # response = model.generate_content(question)\n",
        "\n",
        "    if response.candidates and response.candidates[0].content.parts:\n",
        "        answer = response.candidates[0].content.parts[0].text\n",
        "        #print(answer)\n",
        "        return answer\n",
        "    else:\n",
        "        return \"No answer generated.\"\n",
        "        #return None\n"
      ],
      "metadata": {
        "id": "1Zl32JEEdp8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def complete(video_url):\n",
        "    audio_file = download_audio(video_url)\n",
        "    compressed_audio = compress_audio(input_file = audio_file)\n",
        "    transcript = speech_to_text(compressed_audio)\n",
        "    generate_embeddings(transcript)\n"
      ],
      "metadata": {
        "id": "3qJB_ayq75A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preparation(url):\n",
        "    audio_file = download_audio(url)\n",
        "    compressed_audio = compress_audio(input_file = audio_file)\n",
        "    transcript = speech_to_text(compressed_audio)\n",
        "    return transcript"
      ],
      "metadata": {
        "id": "PAA9vHtxg4gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = \"sshleifer/distilbart-cnn-12-6\""
      ],
      "metadata": {
        "id": "MzfNVeyekXGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_summariser(model_name):\n",
        "    from huggingface_hub import hf_hub_download\n",
        "    model_dir = hf_hub_download(repo_id=model_name, filename=\"config.json\", cache_dir=\"models\")\n",
        "    summarizer = pipeline(\"summarization\", model=model_name, cache_dir=\"models\")\n",
        "    return summarizer\n",
        "\n"
      ],
      "metadata": {
        "id": "P74pwSNRj5PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_tokenizer(model_name):\n",
        "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "    return tokenizer, model\n"
      ],
      "metadata": {
        "id": "_I4t5pLWjFWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def summarize(text, maxSummarylength=1000):\n",
        "#     model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
        "#     tokenizer, model = load_model_and_tokenizer(model_name)\n",
        "#     inputs = tokenizer.encode(\"summarize: \" + text,\n",
        "#                               return_tensors=\"pt\",\n",
        "#                               max_length=1024,\n",
        "#                               truncation=True)\n",
        "#     summary_ids = model.generate(\n",
        "#         inputs,\n",
        "#         max_length=int(maxSummarylength),\n",
        "#         min_length=int(maxSummarylength / 5),\n",
        "#         length_penalty=10.0,\n",
        "#         num_beams=4,\n",
        "#         early_stopping=True\n",
        "#     )\n",
        "#     summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "#     return summary\n",
        "def summarize(text, max_summary_length=200, min_summary_length=50):\n",
        "    model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
        "    tokenizer, model = load_model_and_tokenizer(model_name)\n",
        "\n",
        "    inputs = tokenizer.encode(\n",
        "        \"summarize: \" + text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=1024,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    summary_ids = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_summary_length,\n",
        "        min_length=min_summary_length,\n",
        "        length_penalty=2.0,   # balanced\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary"
      ],
      "metadata": {
        "id": "CAAacCozhU0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def recursive_summarize(text, chunk_size=800):\n",
        "#     words = text.split()\n",
        "#     chunks = [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "#     partial_summaries = []\n",
        "#     for chunk in chunks:\n",
        "#         partial_summaries.append(summarize(chunk))\n",
        "\n",
        "#     return summarize(\" \".join(partial_summaries))\n",
        "def recursive_summarize(text, chunk_size=500):\n",
        "    words = text.split()\n",
        "    chunks = [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "    partial_summaries = [summarize(chunk) for chunk in chunks]\n",
        "\n",
        "\n",
        "    if len(\" \".join(partial_summaries).split()) < 900:\n",
        "        return summarize(\" \".join(partial_summaries), max_summary_length=250)\n",
        "    else:\n",
        "        return \" \".join(partial_summaries)"
      ],
      "metadata": {
        "id": "0oTcvt4MDHMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adaptive_recursive_summarize(text, chunk_size=800, min_words=150, max_words=400):\n",
        "    \"\"\"\n",
        "    Adaptive summarizer that produces summaries of reasonable length automatically.\n",
        "    \"\"\"\n",
        "    # Step 1: Split into chunks\n",
        "    words = text.split()\n",
        "    chunks = [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "    partial_summaries = [summarize(chunk, max_summary_length=max_words) for chunk in chunks]\n",
        "\n",
        "    combined_summary = \" \".join(partial_summaries)\n",
        "\n",
        "    # Step 2: Adaptive refinement\n",
        "    word_count = len(combined_summary.split())\n",
        "\n",
        "    if word_count < min_words:\n",
        "        # Too short → expand by re-summarizing original text with higher length\n",
        "        return summarize(text, max_summary_length=max_words * 2)\n",
        "\n",
        "    elif word_count > max_words * 3:\n",
        "        # Too long → compress again\n",
        "        return summarize(combined_summary, max_summary_length=max_words)\n",
        "\n",
        "    else:\n",
        "        # Good length → return combined\n",
        "        return combined_summary"
      ],
      "metadata": {
        "id": "WPmddsOJ_7vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_functionality(text, loaded):\n",
        "    if not loaded:\n",
        "        model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
        "        download_summariser(model_name)\n",
        "        tokenizer, model = load_model_and_tokenizer(model_name)\n",
        "    return adaptive_recursive_summarize(text)"
      ],
      "metadata": {
        "id": "fXDTpnJRl7_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = preparation(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "5iPsClMcguRE",
        "outputId": "8e6296db-ee63-4827-dea3-44e247ba55c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=1tRTWwZ5DIc\n",
            "[youtube] 1tRTWwZ5DIc: Downloading webpage\n",
            "[youtube] 1tRTWwZ5DIc: Downloading tv client config\n",
            "[youtube] 1tRTWwZ5DIc: Downloading tv player API JSON\n",
            "[youtube] 1tRTWwZ5DIc: Downloading tv simply player API JSON\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3985697532.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreparation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2047394659.py\u001b[0m in \u001b[0;36mpreparation\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreparation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0maudio_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcompressed_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompress_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtranscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeech_to_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompressed_audio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtranscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-722800770.py\u001b[0m in \u001b[0;36mdownload_audio\u001b[0;34m(link, file_name)\u001b[0m\n\u001b[1;32m      4\u001b[0m                            \u001b[0;34m'overwrites'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                            'outtmpl': file_name}) as video:\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0minfo_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mvideo_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m#video.download(link)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/yt_dlp/YoutubeDL.py\u001b[0m in \u001b[0;36mextract_info\u001b[0;34m(self, url, download, ie_key, extra_info, process, force_generic_extractor)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mExistingVideoReached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__extract_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_info_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1658\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m             \u001b[0mextractors_restricted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allowed_extractors'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'default'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/yt_dlp/YoutubeDL.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1666\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1668\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1669\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mCookieLoadError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDownloadCancelled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPagedList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1670\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/yt_dlp/YoutubeDL.py\u001b[0m in \u001b[0;36m__extract_info\u001b[0;34m(self, url, ie, download, extra_info, process)\u001b[0m\n\u001b[1;32m   1801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1803\u001b[0;31m             \u001b[0mie_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUserNotLive\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/yt_dlp/extractor/common.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    760\u001b[0m                     self.to_screen('Extracting URL: %s' % (\n\u001b[1;32m    761\u001b[0m                         url if self.get_param('verbose') else truncate_string(url, 100, 20)))\n\u001b[0;32m--> 762\u001b[0;31m                     \u001b[0mie_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_real_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mie_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/yt_dlp/extractor/youtube/_video.py\u001b[0m in \u001b[0;36m_real_extract\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m   4196\u001b[0m                             f'{trans_name} (Original)', client_name, pot_params)\n\u001b[1;32m   4197\u001b[0m                     \u001b[0;31m# Setting tlang=lang returns damaged subtitles.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4198\u001b[0;31m                     process_language(\n\u001b[0m\u001b[1;32m   4199\u001b[0m                         \u001b[0mautomatic_captions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4200\u001b[0m                         pot_params if orig_lang == orig_trans_code else {'tlang': trans_code, **pot_params})\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/yt_dlp/extractor/youtube/_video.py\u001b[0m in \u001b[0;36mprocess_language\u001b[0;34m(container, base_url, lang_code, sub_name, client_name, query)\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 lang_subs.append({\n\u001b[1;32m   4105\u001b[0m                     \u001b[0;34m'ext'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4106\u001b[0;31m                     \u001b[0;34m'url'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0murljoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.youtube.com'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_url_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4107\u001b[0m                     \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msub_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4108\u001b[0m                     \u001b[0;34m'impersonate'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/yt_dlp/utils/_utils.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequired_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/yt_dlp/utils/_utils.py\u001b[0m in \u001b[0;36mupdate_url_query\u001b[0;34m(url, query)\u001b[0m\n\u001b[1;32m   2602\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mpartial_application\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupdate_url_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2604\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdate_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/yt_dlp/utils/_utils.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequired_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/yt_dlp/utils/_utils.py\u001b[0m in \u001b[0;36mupdate_url\u001b[0;34m(url, query_update, **kwargs)\u001b[0m\n\u001b[1;32m   2593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquery_update\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2594\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;34m'query'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'query_update and query cannot be specified at the same time'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2595\u001b[0;31m         kwargs['query'] = urllib.parse.urlencode({\n\u001b[0m\u001b[1;32m   2596\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_qs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2597\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mquery_update\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/parse.py\u001b[0m in \u001b[0;36murlencode\u001b[0;34m(query, doseq, safe, encoding, errors, quote_via)\u001b[0m\n\u001b[1;32m   1029\u001b[0m                 \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquote_via\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m                 \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquote_via\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d24Qrj0kw--d",
        "outputId": "3db69074-083b-4628-9d10-9794a54a9351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Well, starting tomorrow, work is the rest of your life. That's right. So my first wish, find work that gives you great joy. Jensen is here, of course the CEO of NVIDIA. This is the clear winner of every winner in the world of artificial intelligence. What an amazing year! His company powers everything from OpenAI, Google's programs. You are the stars. This is a celebration of your science, of your work and your innovations. My second wish for you is to learn to embrace failure. You know, once upon a time, NVIDIA was a failing company. Their first product was such a disaster that the customers had completely rejected their products. Microsoft had launched a lethal product that made them absolutely useless. And NVIDIA only had 30 days of cash left. At the time that we started the company in 1993, we were the only consumer 3D graphics company ever created. The year is 1996 and NVIDIA is collapsing. We want to contract with Sega to build their game console, which attracted games for our platform and funded our company. After one year of development, we realized our architecture was the wrong strategy. The condition of the company was so bad that Jensen was skipping electricity bills just to pay his employees. And during this time, the office lights were dimmed only to decrease their electricity bills. And just when things couldn't get any worse, something even more terrible happened. Their biggest client dropped them, their engineers needed to be laid off and the entire Silicon Valley was laughing at NVIDIA. And while NVIDIA was choking under pressure, Intel was making $5 billion every single quarter. Chipmaker Intel has seen its best ever first quarter earnings as its profits quadrupled on the back of increased sales. By 1992, it became the world's largest chipmaker by revenue, according to the company. By 1997, its chips served as the brains of 84% of the world's computers. And Intel alone was so powerful that they could outspend, out-engineer and outlive every rival in the game. But right when NVIDIA was about to die, a miracle happened. Jensen Yuan built a chip. A chip so miraculous that it did not just save NVIDIA, it was a stepping stone to turn NVIDIA into the most powerful company in the world. And today, ladies and gentlemen, NVIDIA is worth $4.2 trillion, which is more than Apple, more than Microsoft and more than the GDP of 185 countries in the world. The price of NVIDIA was yet another record high. A new milestone in the world of artificial intelligence. NVIDIA became the first company hit a market value of $4 trillion market cap. Chad GBT has started a very intense conversation. He thinks it's the most revolutionary, he thinks it's the iPhone. But the question is, when this company was just 30 days away from bankruptcy, how did this $4 trillion miracle happen? In the world of artificial intelligence. The most valuable tech company? NVIDIA. Before we dive in, let's take a moment to thank Odoo for supporting our podcast. Imagine a small manufacturing company in Pune that consistently juggles between multiple software tools like one for inventory, another for accounting and another for sales. This leads to miscommunication, inefficiencies and valuable time is lost in just managing these disconnected systems. But what if all your business needs could be handled in just one place? Well, this is where Odoo comes in as a game changer for businesses of all sizes. Odoo is an all-in-one enterprise resource planning platform that brings together 45 easy to use applications. Whether you are managing your sales, invoicing, project management, inventory or even building a website, Odoo's central focus is on the use of the internet. So no more juggling, no more disconnected tools and no more dealing with messy integrations. Odoo has a very seamless integration. Every app communicates with the other, which creates a unified system. For example, sales is automatically in sync with inventory and accounting, which gives you a real-time overview of your business performance. And Odoo's user-friendly interface ensures that you can easily manage your business. So as your business grows, Odoo grows with you. It's completely scalable and it will allow you to add new applications as your needs evolve. Automation tools streamline repetitive tasks and give you more time to focus on what truly matters, which is scaling your business. So, let's get started. So, let's get started. So, let's get started. So, let's get started. So, let's get started. So, let's get started. The best part, your first Odoo application is completely free for life with unlimited hosting and support included. So if you want to grow your business in a flash, check out the link in the description and start your journey with Odoo today. To understand Nvidia, you need to know what exactly is a chip and why does the world care so much about chips. You see in 2025 when you're holding a phone, watching videos, using AI, making calls or even scrolling through Instagram None of this magic comes from your screen. It comes from something invisible Which is buried deep inside your phone. This object is called a chip It's not just any part. It is literally the brain of everything digital So every time you swipe to unlock, tap to shoot in a video game, zoom on Google Maps or even while watching a YouTube video A chip is doing millions of calculations every single second just to make these functions happen It takes your tap, decodes it, renders it, processes it and then turns it into an action on the screen And the most miraculous thing about chips is that they are faster than a blink of an eye, smaller than your fingernail But they are so powerful that the entire world from Instagram to Jack GPT to Netflix to Amazon to even Apple iPhones Run on these chips, but back in 1993 this was not very obvious At the time the PC revolution was just starting while the major tech companies would be racing to the pinnacle of the PC market From Microsoft software and faster processors to 3d graphics and modern gaming A storm that would change how we work play and live. The world of 1983 was completely different computers were bulky Expensive and mostly boring. So all you could do with these computers was use spreadsheets, Word documents and boring 2d interfaces But that is when something insane happened when the world was introduced to a new concept called video games Now as soon as video games were introduced, games went from flat pixels to 3d worlds This is where the world fell in love with games like Doom and Quake But this is when Jensen and his two co-founders observed two big problems in the industry Firstly they observed that to play these amazing games you needed a separate console like a Super Nintendo or a Sega Genesis Firstly they observed that to play these amazing games you needed a separate console like a Super Nintendo or a Sega Genesis And secondly computers just couldn't handle these games at all They didn't have specialized chips that were needed to run high-speed high-quality graphics because these chips needed to do billions I repeat billions of calculations every single second to make you play a simple game For example when you play Call of Duty you are sprinting through a battlefield while the bullets are flying and explosions are everywhere The lighting changes as you move from a dark hallway all the way into a sunlight and while this happens Shadows shift and smoke reacts to gunfire But behind the scenes the computer is working like crazy The computer is figuring out how bright the sunlight should hit your helmet How shadows fall on the wall behind you and how the granite spins through the air and how it explodes right next to an enemy And all of this movement, physics, lighting and reaction is being calculated in real time The computer does this 60 times per second frame by frame to keep the game smooth and real That's millions of tiny calculations happening at lighting speed Sounds like a lot right? Now you would think you know what's the big deal with a computer making a million calculations It should be simple. Look at this This is Mario 64 in 1996 which needed 100 million calculations every second to run Minecraft in 2011 needed 100 billion calculations per second and new games like Call of Duty Modern Warfare 2 Need over 36 trillion calculations to be done every single second That is what it takes to make every explosion feel real every reload feels smooth and every shot hit just right That's what it takes a computer to give you the perfect headshot and to do all of this back Back then you needed a miraculous chip that can perform billions of calculations per second But the problem was that back in 1995 people would laugh at games thinking it was a kiddish domain Everyone saw games as toys for kids In fact, the entire tech world was busy chasing business software servers and spreadsheets Because of which VCs turned these game companies away Engineers did not want to join game companies and the industry saw it as a complete waste of talent So the question is when the entire world was against games, why was Jensen crazy enough to build chips for games? Well, this is what reminds me of something very wise that Anupam Mittal told me at the podcast He said that a great entrepreneur is defined by the most non-obvious Insight he has about the industry that nobody else believes in. I repeat a great entrepreneur is defined by the most Non-obvious insight he has about the industry that no one else believes in. In this case while everyone discarded games Jensen believed something the world hadn't realized yet While the world mocked games Jensen was watching the math behind this madness When others saw fun Jensen saw the future in gaming because games demanded speed They required higher resolution graphics and they had to run millions of calculations with no lag and no delay So Jensen knew that if they could only build chips that were powerful enough to run games smoothly They could build chips for anything if their chips could survive the chaos of a video game They could thrive in any field from rockets to robotics from medicine to movies That is how Nvidia saw chips as a gateway to the future of computing That is how ladies and gentlemen in 1995 Nvidia launched their first product a computer chip called Nv1 The Nv1 chip that was released in November 1995 sound blaster compatible audio systems and 15 pin joystick boards To top it off. It would be compatible with the Sega Saturn console that was about to hit the market Stars were aligned for it This chip was supposed to be magical Because this one chip alone could handle everything from graphics to sound and even came controls and back then Sega The Japanese gaming giant partnered with Nvidia and bundled their new PC game controller with Nvidia's chip So when this collaboration happened to the entire industry, it looked like the perfect match There was a giant gaming company and there was a chip manufacturer who was being supported by the giant gaming company So it couldn't be any better But this is when something terrible happened Like I said in the intro Nvidia Nv1 failed so miserably that Nvidia had to beg Sega to break the partnership and things got so bad That out of the 250,000 units they sold 249,000 units came back from the distribution partners The question is why this was supposed to be the perfect match in Silicon Valley, right? Well, the reason was Microsoft Microsoft launched something called DirectX One example of this was Microsoft's DirectX. It's the most powerful and efficient graphics API Microsoft's DirectX APIs only supported triangle primitives DirectX was a set of rules and tools created by Microsoft in 1995, especially for game developers So DirectX was Microsoft's way of telling the game developers that if you want your games to work on Windows PCs You'll have to follow these instructions and the first rule was to use triangles to draw everything now I know this sounds a little complicated. So let's break this down using a game that we all love which is Call of Duty In Call of Duty when you see a soldier running the gun in his hand the helmet on his head and the buildings around him All of them are built using thousands of tiny triangles Triangles are like digital Lego blocks stack enough of them and you will get a face a weapon and even a whole battlefield These are called low poly models which are made up of simple triangles and optimized for speed But Nvidia said let's keep triangles and use curves or Quadratic surface textures to make everything look smoother in other words Nvidia wanted to build high poly models Which were rich fluid and realistic? But the problem was that Microsoft said if you want your games to work on our PCs you must use triangles and not curves Because DirectX which was Microsoft's new graphic standard it was built entirely on low poly triangle based rendering So if game developers had to build games for Nvidia they had to develop everything from scratch It's almost like saying you built an app for iOS when the platform you need to launch it on is Android That is what happened to Nvidia when they built a graphics chip NV one that did not follow DirectX That's what happened to Nvidia when they built the graphics chip NV one which did not follow DirectX instructions So if you think about it, this wasn't just a tech disagreement. This was war Microsoft had locked the doors and Nvidia had built the wrong key So the game developers started ignoring nv1 and Nvidia was left with 249,000 useless chips This is the reason why Nvidia practically had to beg Sega to break the contract Eventually their distributors returned the product and Nvidia was 30 days away from bankruptcy So the question is what exactly did Jensen do and how did he turn Nvidia into this trillion dollar miracle? Well, you know what somehow this crazy guy gathered his remaining engineers and said guys We are going to build a new chip and we are going to do it in record time But this time we will not test it on hardware You know how crazy this move was in the chip industry There is one rule always test before you manufacture because a modern-day chip isn't just a piece of metal It is a microscopic city with billions of transistors millions of logic gates and thousands of interconnected pathways It's like building a skyscraper with 100 billion electrical switches now imagine you cannot touch the wires You cannot see the switches and you get only one shot to get the wiring right and testing is super super critical Because even if one connection is misrouted the whole chip will fail If one logic gate is out of place the chip will overheat and crash and the cost of this failure is not thousands of dollars It is millions of dollars because when you send a chip designed to the factory They don't make you one chip. They will manufacture a million chips So even if there is just one bug you would have just mass-produced a million broken chips But even then Jensen sent his chips to TSMC without physically testing it. It was a mad man's decision In fact, all Jensen did was simulate the entire chip in software and tested it only virtually almost like trying to launch a rocket But only in a 3d simulation software if the software was wrong even by a nanosecond The chip would be dead on arrival and Nvidia would be finished So after sending the chips for manufacturing Jensen and his team waited for eight long weeks Nobody could sleep and every day they would talk about the same thing. Did we make a mistake? Did we do everything right and what if he missed something and Then finally after eight weeks the product arrived and with their heart pounding the team loaded it and hit power on And ladies and gentlemen what they saw was a miracle and this miracle is something that Jensen will never forget Guess what the chip worked perfectly No bugs, no glitches and the graphics rendered beautifully games ran smoothly Frame rates flew and the developers fell in love with this chip. This chip was called Riva 128. Nvidia designed its first high performance graphics chip in 1997 The Riva 128 which stands for real-time interactive video and animation accelerator was a pretty impressive GPU Riva 128 is the world's first fully accelerated hardware Accelerated pipeline for the rendering 3d and once the reviewers used Riva 128 They loved it so much that they screened to the world that Nvidia had built something extraordinary And that is how the order started coming and Riva 128 started selling it sold so well That within the first four months of launch Nvidia had shipped one million units More than any other chip maker in the world in the same span This wasn't just a comeback. It was the rebirth of Nvidia, but as usual again, there was one big problem This chip alone wasn't enough to make Nvidia the 4.2 trillion dollar company that it is today because remember Silicon Valley in 1997 was booming with hundreds of startups with millions of dollars in funding So the question is how did Nvidia beat the competition and how did it become the AI God that it is today? Well after the miracle chip Riva 128 launched Nvidia became the undisputed king of 3d graphics acceleration And while competitors were busy catching up Jensen pushed even harder in 1999 Nvidia launched a chip called the GeForce 256 the world's first GPU or graphics processing unit. This is where Nvidia started the parallel processing revolution And here's a video to help you understand the difference between CPU and GPU better Okay, GPU painting demonstration And 10 9 8 7 6 5 4 3 2 1 So long story short while a CPU handles a few complex tasks at a time a GPU can handle thousands of simple tasks simultaneously And in games where you need every shadow every bullet and every explosion rendered in real time. This was a superpower But you know what guys? Jensen did not stop there because Jensen had another non-obvious insight of the world that no one was thinking about You know what that was? While most people thought GPUs were built to make games look better Jensen saw something the world didn't he realized that the same chip that makes a bullet fly in a game could also calculate How a rocket lands on Mars it could also simulate how cells behave in the human body or even train a machine? To think like a human so where others saw a graphics chip Jensen saw a new kind of brain a brain built not just for games, but for rockets medicines and something called artificial intelligence and that is when Nvidia stopped being a chip company and started becoming the future of computing So in 2006 Jensen did something that most CEOs would never do even in their wildest dreams He invested all his profits to build something that didn't even have a market yet This product was a software platform called CUDA for non techies I have to tell you that CUDA or computer unified device architecture was this magical programming model that unlocked the full power of Nvidia chips So before CUDA GPUs were mostly used for graphics, but with CUDA developers could now use them for anything for example with CUDA a scientist could simulate disease outbreaks in seconds a Self-driving car could see a child running into traffic and stop and NASA could calculate a rocket's path to Mars in real time So in short CUDA turned Nvidia chips from gaming tools into world-changing engines So magical right after developing CUDA within a few years Nvidia should have become a billion trillion dollar company isn't it? Well guess what? It did not you know why because CUDA was 10 years ahead of its time Yes, CUDA was 10 years ahead so far ahead that it was almost like inventing the rocket engine before humans even wanted to go to space So CUDA was powerful. It was futuristic, but nobody knew what to do with it In fact in 2006 AI wasn't even ready. There was no charge GBT. No concept of self-driving cars No billion parameter models at all and the world was still using simple algorithms that did not need massive computing power at all So it's like people were solving math problems with calculators and Jensen somehow gave them a supercomputer What will they do with a supercomputer when they were just using a calculator? You get this right so it was powerful, but nobody needed it Similarly AI research was still small and experimental so researchers did not have the data The models or even the belief that deep learning would work so CUDA was ready But the world wasn't ready for CUDA so for six years I repeat for six years nothing happened And then again a miracle happened you have Jeffrey Hinton the godfather of AI whose controversial ideas help make Advanced artificial intelligence possible computers were down a lot faster and now it's behaving like I thought it would behave in the mid-ages It's solving everything the arrival of super fast chips and the massive amounts of data produced on the internet gave Hinton's algorithms a magical boost in 2012 in a quiet lab in the university of Toronto Young researchers named Alex and Ilya and their mentor Jeffrey Hinton did something absolutely insane They had a dream a dream to build a computer brain that could see like a human So if you give the computer an image They wanted the computer to tell you if it was a dog or a cat a car or a truck a hospital room or a battlefield But here's the twist they did not use a giant supercomputer from NASA as it turns out They used an Nvidia chip and it was powered by Nvidia CUDA so this computer was fed with millions of images of faces animals cars in cities and Somehow the GPU did not just crunch numbers it started learning and eventually the computer learned How to see and this AI model was called Alex net and it entered the biggest AI competition in the world on 30th of September 2012 and you know this day will be remembered in history as the day the world fell in love with Nvidia You know why because Alex net did not just win the biggest AI competition it literally Obliterated its competition in 2012 a group of three researchers Submits an entry to a famous competition where the goal is to create computer systems that could recognize images and label them with categories And their entry just crushes the competition it gets way fewer answers wrong. It was incredible. It blows everyone away It's called Alex net and it's a kind of AI called a neural network and they did it on Nvidia GPUs All of a sudden GPUs weren't just a way to make computers faster and more efficient They're becoming the engines of a whole new way of computing and this was a big big deal because Before Alex net AI was considered to be only theoretical. It was slow clunky and disappointing But Alex net proved three very important things to the world number one They proved that AI can learn if you give it enough data Secondly, they proved that GPUs are the perfect brain to train these models and last and most importantly They proved that CUDA was the secret sauce that made it possible and suddenly the entire world realized what? Jensen had seen six years earlier This was the spark that lead the AI revolution in the world because after Alex net Google built better search and translation Facebook improved face recognition Tesla started training self-driving cars and open AI began building large language models that would then be known as chat GPT And guess what all these companies and projects had two things in common in their back end They all used Nvidia GPUs and they all used Nvidia's CUDA This wasn't just a win for Jensen This was the birth of the modern AI revolution and when all these giants started using Nvidia GPUs Nvidia became a four trillion dollar company and that is how Jensen turned a company that was 30 days away from bankruptcy into a four trillion dollar miracle Eventually Nvidia became the most powerful company on earth I try to spend my time as much as I can on The things that I believe will have long lasting influence our company I wish for you to love your work as much as I've loved my work the nearly 6,000 employees at Nvidia and Everybody if I've ever worked with in the computer industry knows I love my work Nvidia is my life's work one that I'm passionate about and one that I'm giving my very best to make as great as possible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_url(text):\n",
        "    loaded = False\n",
        "    summary = summarize_functionality(text, loaded)\n",
        "    loaded = True\n",
        "    return summary"
      ],
      "metadata": {
        "id": "GP48PgFGfsZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(summarize_url(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8K3JzFbDlYIM",
        "outputId": "1f9808a2-4f88-4ec9-df3b-0368bdb81266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Odoo is an all-in-one enterprise resource planning platform that brings together 45 easy to use applications . The company is worth $4.2 trillion, more than Apple and more than the GDP of 185 countries in the world . Odoo's user-friendly interface ensures that you can easily manage your business .  To understand Nvidia, you need to know what exactly is a chip and why does the world care so much about chips . Back in 1993 the PC revolution was just starting while the major tech companies would be racing to the pinnacle of the PC market From Microsoft software and faster processors to 3d graphics and modern gaming, Nvidia would change how we work play and live .  In 1995 Nvidia launched their first product a computer chip called Nv1 The chip that was released in November 1995 sound blaster compatible audio systems and 15 pin joystick boards To top it off, it would be compatible with the Sega Saturn console that was about to hit the market . But this is when something terrible happened . Microsoft launched something called DirectX which was Microsoft's new graphic standard it was built entirely on triangle based rendering .  Nvidia designed its first high performance graphics chip in 1997 The Riva 128 which stands for real-time interactive video and animation accelerator was a pretty impressive GPU . But as usual again, there was one big problem this chip alone wasn't enough to make Nvidia the 4.2 trillion dollar company that it is today . Even if there is just one bug you would have just mass-produced a million broken chips .  In 2006 Jensen invested all his profits to build a software platform called CUDA for non techies . CUDA turned Nvidia chips from gaming tools into world-changing engines . It was futuristic, but nobody knew what to do with it . In 2006 AI research was still small and experimental so researchers did not have the data so CUDA was ready . In 2012 a group of three researchers created a kind of AI called a neural network and they did it on Nvidia GPUs . All of a sudden they weren't just a way of making computers faster and more efficient . They're becoming the engines of a whole new way of a new way .  Alex net proved that AI can learn if you give it enough data . Nvidia's CUDA was the secret sauce that made it possible . This was the spark that lead the AI revolution in the world . After Alex net Google built better search and translation . Facebook improved face recognition . Tesla started training self-driving cars and open AI began building large language models .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db = generate_embeddings(text)"
      ],
      "metadata": {
        "id": "pYMgyfrvhcUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"tell me about the rise on nvidia\"\n"
      ],
      "metadata": {
        "id": "L17Yx320htUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def qna_functionality(question):\n",
        "    res = closest(question, db)\n",
        "    answer = answer__llm(question, res)\n",
        "    # if answer !=None:\n",
        "    print(answer)\n"
      ],
      "metadata": {
        "id": "b6Wo-O6kgV83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qna_functionality(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "rwh79t7mm-Je",
        "outputId": "7d872d2d-2928-4eba-cd06-9ad8496e0256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the context provided, here is a summary of the rise of Nvidia:\n",
            "\n",
            "Nvidia was on the brink of failure when a \"miracle\" happened: Jensen Yuan built a chip called the Riva 128 in 1997. This chip, which stands for \"real-time interactive video and animation accelerator,\" was the world's first fully hardware-accelerated pipeline for 3D rendering. Games ran smoothly on it, and both developers and reviewers loved it, leading to a surge in orders. The Riva 128 sold so well that Nvidia shipped one million units in the first four months, more than any other chipmaker in that period. This success marked the \"rebirth of Nvidia\" and made it the \"undisputed king of 3d graphics acceleration.\"\n",
            "\n",
            "To stay ahead of the competition, Nvidia pushed harder and in 1999 launched the GeForce 256, the world's first GPU (graphics processing unit). This new chip started the \"parallel processing revolution,\" as a GPU can handle thousands of simple tasks simultaneously. This journey was a stepping stone that turned Nvidia into a company worth $4.2 trillion, the first to hit a $4 trillion market cap.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for m in genai.list_models():\n",
        "#     print(m.name, m.supported_generation_methods)"
      ],
      "metadata": {
        "id": "i8zCPre-nIgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ld4diIiMstVF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
